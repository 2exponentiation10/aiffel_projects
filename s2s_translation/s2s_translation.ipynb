{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff01aaef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.3\n",
      "2.6.0\n",
      "3.4.3\n",
      "완료!\n"
     ]
    }
   ],
   "source": [
    "# 한국어 적용과 시각화설정\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(pd.__version__)\n",
    "print(tf.__version__)\n",
    "print(mpl.__version__)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1439e9d",
   "metadata": {},
   "source": [
    "# 데이터 준비\n",
    "프로젝트 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3eacc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eed03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd936b5d",
   "metadata": {},
   "source": [
    "# 데이터 전처리 \n",
    "## 1. 정제\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0900e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ko = os.path.join(current_dir, \"korean-english-park.train.ko\")\n",
    "path_to_en = os.path.join(current_dir, \"korean-english-park.train.en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33246e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 데이터 크기: 94123\n",
      "영어 데이터 크기: 94123\n",
      "한국어 예시 데이터:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.\n",
      ">> 그러나 이것은 또한 책상도 필요로 하지 않는다.\n",
      ">> 79.95달러하는 이 최첨단 무선 광마우스는 허공에서 팔목, 팔, 그외에 어떤 부분이든 그 움직임에따라 커서의 움직임을 조절하는 회전 운동 센서를 사용하고 있다.\n",
      ">> 정보 관리들은 동남 아시아에서의 선박들에 대한 많은 (테러) 계획들이 실패로 돌아갔음을 밝혔으며, 세계 해상 교역량의 거의 3분의 1을 운송하는 좁은 해로인 말라카 해협이 테러 공격을 당하기 쉽다고 경고하고 있다.\n",
      ">> 이 지역에 있는 미국 선박과 상업용 선박들에 대한 알카에다의 (테러) 시도 중 여러 건이 실패했다는 것을 알게 된 후에, 전문가들은 테러 조직이 여전히 세계 경제에 타격을 입히려 한다고 경고하고 있으며, 동남 아시아에 있는 세계 경제의 주요 통로가 위험에 처해 있다고 그들은 생각하고 있다.\n",
      ">> 국립 과학 학회가 발표한 새 보고서에따르면, 복잡한 임무를 수행해야 하는 군인들이나 보다 오랜 시간 동안 경계를 늦추지 않고 있기 위해 도움이 필요한 군인들에게 카페인이 반응 시간을 증가시키고 임무 수행 능력을 향상시키는데 도움이 된다고 한다.\n",
      ">> 이 보고서에따르면, \"특히, 군사 작전에서 생사가 걸린 상황이 될 수도 있는 반응 속도와 시각 및 청각의 경계 상태를 유지시키기 위해 카페인이 사용될 수도 있다.\" 고 한다.\n",
      ">> \"결정적인 순간에 그들의 능력을 증가시켜 줄 그 무엇이 매우 중요합니다.\"\n",
      ">> 연구가들이 이미 커피 대체품으로서 음식 대용 과자나 껌에 카페인을 첨가하는 방법을 연구하고 있다고 Archibald는 말했다.\n",
      "영어 예시 데이터:\n",
      ">> Much of personal computing is about \"can you top this?\"\n",
      ">> so a mention a few weeks ago about a rechargeable wireless optical mouse brought in another rechargeable, wireless mouse.\n",
      ">> Like all optical mice, But it also doesn't need a desk.\n",
      ">> uses gyroscopic sensors to control the cursor movement as you move your wrist, arm, whatever through the air.\n",
      ">> Intelligence officials have revealed a spate of foiled plots on ships in Southeast Asia and are warning that a narrow stretch of water carrying almost one third of the world's maritime trade is vulnerable to a terror attack.\n",
      ">> After learning of several foiled al Qaeda attempts on U.S. and commercial ships in the area, experts are warning that the terror network still wants to cripple the global economy, the world's economic jugular vein in Southeast Asia is at risk.\n",
      ">> Caffeine can help increase reaction time and improve performance for military servicemen who must perform complex tasks or who need help staying alert for longer periods of time, according to a new report by the National Academy of Sciences.\n",
      ">> \"Specifically, it can be used in maintaining speed of reactions and visual and auditory vigilance, which in military operations could be a life or death situation,\" according to the report.\n",
      ">> \"Something that will boost their capabilities at crucial moments is very important.\"\n",
      ">> Researchers are already exploring ways to put caffeine in nutrition bars or chewing gum as alternatives to coffee, Archibald said.\n",
      "전처리 후 한국어 예시 데이터: ['만명이 직간접 영향을 받고 명 이상이 사망한 초강력 사이클론 시드르 가 방글라데시를 강타한지 달이 지났다 .', '오바마의 선거전략 책임자인 데이빗 엑셀로드는 말하자면 , 우리는 사람들이 우리에게 호의적이지 않을 때 나온 여론조사 결과에 낙심하지 않고 현재 여론조사에도 현혹되지 않는다 며 마지막 날까지 매일이 전쟁이 될 것이고 이에 대비하고 있다 고 밝혔다 .', '이 대통령은 이날 이계훈 합동참모본부 차장 등 군 중장 진급자들로부터 진급 및 보직 신고를 받는 자리에서 북한의 긴장조성은 바람직하지 않다 는 입장을 밝혔다 .', '이탈리아 대표팀에서 복귀한 안토니오 디 나탈레가 분 유벤투스의 골키퍼 잔루이지 부폰을 따돌린 뒤 결승골을 성공시켰다 .', '리퍼블릭 윈도우 앤 도어스 Republic Windows and Doors 에서 해고된 미국 전기 , 라디오 , 기계 노총 소속 노동자 여명은 갑작스럽게 해고 통지를 받은 지난 일부터 이 회사 공장을 점거해 평화적으로 농성하고 있다고 주장했다 .']\n",
      "전처리 후 영어 예시 데이터: ['<start> More than a month after Cyclone Sidr tore across Bangladesh killing more than , people and affecting at least . million , photojournalist John Cobb visited badly hit areas and contributed this story for CNN . <end>', '<start> Let me just say this , we weren t discouraged by polls when they were not favorable for us , we re not seduced by polls now , said David Axelrod , Obama s senior strategist . <end>', '<start> Lee called North Korea s threats undesirable on Thursday during a meeting of the country s ranking generals , Yonhap reported . <end>', '<start> Fresh from a two goal performance for Italy s national team , Antonio Di Natale headed past Juventus goalkeeper Gianluigi Buffon in the th minute . <end>', '<start> About workers from the United Electrical , Radio and Machine Workers of America have conducted what they call a peaceful occupation of the Republic Windows and Doors plant since Friday , when the abruptly announced layoffs were supposed to take effect . <end>']\n",
      "필터링 후 한국어 데이터 크기: 70171\n",
      "필터링 후 영어 데이터 크기: 70171\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "# 파일 경로 설정\n",
    "current_dir = os.getcwd()\n",
    "path_to_ko = os.path.join(current_dir, \"korean-english-park.train.ko\")\n",
    "path_to_en = os.path.join(current_dir, \"korean-english-park.train.en\")\n",
    "\n",
    "# 파일 내용 읽기\n",
    "with open(path_to_ko, 'r', encoding='utf-8') as f:\n",
    "    kor_data = f.read().splitlines()\n",
    "\n",
    "with open(path_to_en, 'r', encoding='utf-8') as f:\n",
    "    eng_data = f.read().splitlines()\n",
    "\n",
    "# 데이터 크기와 예시 출력\n",
    "print(\"한국어 데이터 크기:\", len(kor_data))\n",
    "print(\"영어 데이터 크기:\", len(eng_data))\n",
    "print(\"한국어 예시 데이터:\")\n",
    "for sen in kor_data[:10]:\n",
    "    print(\">>\", sen)\n",
    "\n",
    "print(\"영어 예시 데이터:\")\n",
    "for sen in eng_data[:10]:\n",
    "    print(\">>\", sen)\n",
    "\n",
    "# 중복 데이터 제거\n",
    "cleaned_corpus = list(set(zip(kor_data, eng_data)))\n",
    "\n",
    "# 전처리 함수 정의\n",
    "mecab = Mecab()\n",
    "\n",
    "def preprocess_sentence_korean(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    # 특수 문자 제거\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    \n",
    "    # 한글과 영어를 제외한 문자 제거\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "# 병렬 데이터 전처리\n",
    "kor_sentences = []\n",
    "eng_sentences = []\n",
    "\n",
    "for kor, eng in cleaned_corpus:\n",
    "    kor_sentences.append(preprocess_sentence_korean(kor))\n",
    "    eng_sentences.append(preprocess_sentence_korean(eng, s_token=True, e_token=True))\n",
    "\n",
    "print(\"전처리 후 한국어 예시 데이터:\", kor_sentences[:5])\n",
    "print(\"전처리 후 영어 예시 데이터:\", eng_sentences[:5])\n",
    "\n",
    "# 길이가 40 이하인 데이터 필터링\n",
    "def filter_long_sentences(kor, eng, max_len=40):\n",
    "    filtered_kor = []\n",
    "    filtered_eng = []\n",
    "    \n",
    "    for k, e in zip(kor, eng):\n",
    "        if len(k.split()) <= max_len and len(e.split()) <= max_len:\n",
    "            filtered_kor.append(k)\n",
    "            filtered_eng.append(e)\n",
    "    \n",
    "    return filtered_kor, filtered_eng\n",
    "\n",
    "filtered_kor, filtered_eng = filter_long_sentences(kor_sentences, eng_sentences)\n",
    "\n",
    "print(\"필터링 후 한국어 데이터 크기:\", len(filtered_kor))\n",
    "print(\"필터링 후 영어 데이터 크기:\", len(filtered_eng))\n",
    "from konlpy.tag import Mecab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e6a62",
   "metadata": {},
   "source": [
    "## 2. 토큰화\n",
    "### 정제된 텍스트를 아래 tokenize() 함수를 사용해 토큰화하고 텐서로 변환하세요. 그리고 변환된 텐서를 80%의 훈련 데이터와 20%의 검증 데이터로 분리하세요! (Tokenizer의 단어 수는 자유롭게 진행하세요!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5319c18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "def enc_tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000,\n",
    "                                                      filters='')\n",
    "    mecab = Mecab()\n",
    "    tokenized_corpus = []\n",
    "    \n",
    "    for sen in corpus:\n",
    "        tokenized_sen = mecab.morphs(sen) #토큰화\n",
    "        tokenized_corpus.append(tokenized_sen)\n",
    "        \n",
    "    tokenizer.fit_on_texts(tokenized_corpus)\n",
    "    \n",
    "    sequences = tokenizer.texts_to_sequences(tokenized_corpus)  # 정수 인코딩\n",
    "    padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, \n",
    "                                                                     padding='post')  # 패딩\n",
    "\n",
    "    return padded_sequences, tokenizer\n",
    "\n",
    "\n",
    "#영어 토큰화 함수\n",
    "def dec_tokenize(corpus):\n",
    "    #토크나이저 객체생성, filters = ' ' :특수문자 필터링 비활성황 \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=15000,\n",
    "                                                      filters='')\n",
    "    \n",
    "    #텍스트 말뭉치(corpus)에 등장하는 모든 단어를 분석하고 각 단어에 고유한 정수 인덱스를 부여\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "    #텍스트 말뭉치를 정수 시퀀스로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, \n",
    "                                                           padding='post')\n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "enc_train, enc_tokenizer = enc_tokenize(kor_sentences)\n",
    "dec_train, dec_tokenizer = dec_tokenize(eng_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4812988",
   "metadata": {},
   "source": [
    "# 모델설계\n",
    "### Encoder는 모든 Time-Step의 Hidden State를 출력으로 갖고, Decoder는 Encoder의 출력과 Decoder의 t-1 Step의 Hidden State로 Attention을 취하여 t Step의 Hidden State를 만들어 냅니다.Decoder에서 t Step의 단어로 예측된 것을 실제 정답과 대조해 Loss를 구하고, 생성된 t Step의 Hidden State는 t+1 Step의 Hidden State를 만들기 위해 다시 Decoder에 전달됩니다.여기서 't=1 일 때의 Hidden State는 어떻게 정의할 것인가?' 가 궁금하실 수 있는데요, 일반적으로 Encoder의 Final State를 Hidden State로 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "995e1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fc4eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, dropout_rate=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        output, state = self.gru(x)\n",
    "        return output, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96320ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, dropout_rate=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vec, 1), x], axis=-1)\n",
    "        \n",
    "        x, h_dec = self.gru(x)\n",
    "        x = self.dropout(x)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x, h_dec, attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33a6cd9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_344/2110761815.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0msample_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_enc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Encoder Output:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msample_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(kor_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(eng_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fb952",
   "metadata": {},
   "source": [
    "# Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "18d1e8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return tf.reduce_mean(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd3f714",
   "metadata": {},
   "source": [
    "### Optimizer는 모델이 학습할 때에 정답을 찾아가는 방법\n",
    "\n",
    "### fit() 함수를 사용할 수 없는 이유는 바로 Loss 함수 때문\n",
    "\n",
    "### Encoder-Decoder 구조는 학습 과정이 일반적이지 않으므로 직접 Loss를 커스텀해서 사용\n",
    "\n",
    "SparseCategoricalCrossentropy() 함수는 모델이 출력한 확률 분포와 (One-hot이 아닌) 정수 인덱스 답안을 비교해 Cross Entropy값을 구해줍니다. CategoricalCrossentropy()라면 [ 0.1, 0.2, 0.7 ] 과 One-hot 인코딩된 라벨 [0, 0, 1] 을 비교하여 점수를 채점하겠지만, SparseCategoricalCrossentropy() 함수라면 [ 0.1, 0.2, 0.7 ] 과 정수 인덱스 답안 2 를 비교하여 점수를 채점하는 거죠. from_logits 는 확률 분포가 Softmax를 거쳐서 들어오는지, 모델의 출력값 그대로 들어오는지를 결정합니다. 우리는 True 로 줬으니 모델의 출력값을 그대로 전달하면 됩니다!\n",
    "\n",
    "### 패딩에 대한 처리를 해주지 않으면 \"PAD\" 토큰만을 생성할 확률이 굉장히 높아집니다.\n",
    "\n",
    "mask 는 정답지에서 'PAD' 토큰을 찾아내어 그 부분에 대한 Loss는 구하지 않도록 하는 역할을 해주죠. equal() 함수에 정확히는 0 이 아닌 <PAD> 토큰의 인덱스를 전달하는 것이 맞지만 대부분의 경우는 0으로 패딩되기 때문에 편의상 0을 전달하여 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645957b",
   "metadata": {},
   "source": [
    "# 훈련\n",
    "\n",
    "train_step()은 학습에 필요한 것을 모두 가져가 Loss를 계산한 후 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15c877c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input, target, encoder, decoder, optimizer, eng_tokenizer):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 인코더 처리\n",
    "        enc_output, enc_hidden = encoder(input, encoder.initialize_hidden_state())\n",
    "\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([eng_tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "\n",
    "        # 디코더 처리\n",
    "        for t in range(1, target.shape[1]):\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(target.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936ade29",
   "metadata": {},
   "source": [
    "@tf.function 데코레이터는 훈련 외적인 텐서플로우 연산을 GPU에서 동작하게 해 훈련을 가속할 수 있도록 도와줍니다.\n",
    "\n",
    "tf.GradientTape()는 학습하며 발생한 모든 연산을 기록하는 테이프입니다. 이것은 모델이 각 스텝의 최종 단계에서 미분값을 구하는 데에 사용\n",
    "\n",
    "1. Encoder에 소스 문장을 전달해 컨텍스트 벡터인 enc_out 을 생성\n",
    "\n",
    "2. t=0일 때, Decoder의 Hidden State는 Encoder의 Final State로 정의. h_dec = enc_out[:, -1]\n",
    "3. Decoder에 입력으로 전달할 'start' 토큰 문장 생성\n",
    "4. 'start' 문장과 enc_out, Hidden State를 기반으로 다음 단어(t=1)를 예측. pred\n",
    "5. 예측된 단어와 정답 간의 Loss를 구한 후, t=1의 정답 단어를 다음 입력으로 사용 (예측 단어 X)\n",
    "6. 반복!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4033f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1:   4%|▍         | 44/987 [01:09<24:49,  1.58s/it, Loss 4.1294]  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /tmp/ipykernel_344/3380621219.py:6 train_step  *\n        enc_output, enc_hidden = encoder(input, encoder.initialize_hidden_state())\n    /tmp/ipykernel_344/3684012298.py:15 call  *\n        output, state = self.gru(x, initial_state=hidden)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent.py:716 __call__  **\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:443 call\n        last_output, outputs, runtime, states = self._defun_gru_call(\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:520 _defun_gru_call\n        last_output, outputs, new_h, runtime = gru_with_backend_selection(\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:820 gru_with_backend_selection\n        last_output, outputs, new_h, runtime = defun_standard_gru(**params)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3038 __call__\n        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3463 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3298 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1007 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:597 standard_gru\n        last_output, outputs, new_states = backend.rnn(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/backend.py:4354 rnn\n        output_time_zero, _ = step_function(\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:589 step\n        z = tf.sigmoid(x_z + recurrent_z)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n        ret = Operation(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 35 and 64 for '{{node add}} = AddV2[T=DT_FLOAT](split, split_1)' with input shapes: [35,1024], [64,1024].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_344/1504150558.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# train_step 호출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         batch_loss = train_step(input_batch,\n\u001b[0m\u001b[1;32m     23\u001b[0m                                 \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                                 \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3036\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m-> 3038\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3298\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /tmp/ipykernel_344/3380621219.py:6 train_step  *\n        enc_output, enc_hidden = encoder(input, encoder.initialize_hidden_state())\n    /tmp/ipykernel_344/3684012298.py:15 call  *\n        output, state = self.gru(x, initial_state=hidden)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent.py:716 __call__  **\n        return super(RNN, self).__call__(inputs, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/engine/base_layer.py:1037 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:443 call\n        last_output, outputs, runtime, states = self._defun_gru_call(\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:520 _defun_gru_call\n        last_output, outputs, new_h, runtime = gru_with_backend_selection(\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:820 gru_with_backend_selection\n        last_output, outputs, new_h, runtime = defun_standard_gru(**params)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3038 __call__\n        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3463 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/eager/function.py:3298 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1007 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:597 standard_gru\n        last_output, outputs, new_states = backend.rnn(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/keras/backend.py:4354 rnn\n        output_time_zero, _ = step_function(\n    /opt/conda/lib/python3.9/site-packages/keras/layers/recurrent_v2.py:589 step\n        z = tf.sigmoid(x_z + recurrent_z)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1383 binary_op_wrapper\n        raise e\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1367 binary_op_wrapper\n        return func(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1700 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:464 add_v2\n        _, _, _op, _outputs = _op_def_library._apply_op_helper(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:748 _apply_op_helper\n        op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:599 _create_op_internal\n        return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3561 _create_op_internal\n        ret = Operation(\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:2041 __init__\n        self._c_op = _create_c_op(self._graph, node_def, inputs,\n    /opt/conda/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1883 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 35 and 64 for '{{node add}} = AddV2[T=DT_FLOAT](split, split_1)' with input shapes: [35,1024], [64,1024].\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, kor_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        # 마지막 배치가 BATCH_SIZE보다 작은 경우를 처리\n",
    "        batch_end_idx = min(idx + BATCH_SIZE, kor_tensor.shape[0])\n",
    "        \n",
    "        # 배치 슬라이싱\n",
    "        input_batch = kor_tensor[idx:batch_end_idx]\n",
    "        target_batch = eng_tensor[idx:batch_end_idx]\n",
    "        \n",
    "        # train_step 호출\n",
    "        batch_loss = train_step(input_batch,\n",
    "                                target_batch,\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                eng_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd98083f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Attention 시각화 함수\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818945ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 함수\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 훈련 루프\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "EPOCHS = 10\n",
    "sample_sentence = \"안녕하세요. 너무 피곤해 죽을것같아요!\"\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_tensor.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_tensor[idx:idx+BATCH_SIZE],\n",
    "                                dec_tensor[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    # 매 에포크마다 예문을 번역하여 출력\n",
    "    print(f\"\\nEpoch {epoch + 1} 번역 결과:\")\n",
    "    translate(sample_sentence, encoder, decoder)\n",
    "\n",
    "print(\"훈련 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38950a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어진 예문\n",
    "sentences = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    translate(sentence, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c5e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
